{
    "abstract": "<p>\nThis paper addresses the problem of choosing a kernel suitable for\nestimation with a support vector machine, hence further automating\nmachine learning.  This goal is achieved by defining a reproducing\nkernel Hilbert space on the space of kernels itself. Such a\nformulation leads to a statistical estimation problem similar to\nthe problem of minimizing a regularized risk functional.\n</p> \n<p>\nWe state the equivalent representer theorem for the choice of kernels\nand present a semidefinite programming formulation of the resulting\noptimization problem. Several recipes for constructing hyperkernels\nare provided, as well as the details of common machine learning\nproblems. Experimental results for classification, regression and\nnovelty detection on UCI data show the feasibility of our approach.\n</p>",
    "authors": [
        "Cheng Soon Ong",
        "Alexander J. Smola",
        "Robert C. Williamson"
    ],
    "id": "ong05a",
    "issue": 36,
    "pages": [
        1043,
        1071
    ],
    "title": "Learning the Kernel with Hyperkernels",
    "volume": "6",
    "year": "2005"
}