{
    "abstract": "This paper is concerned with the construction and analysis of a\nuniversal estimator for the regression problem in supervised learning.\nUniversal means that the estimator does not depend on any a priori\nassumptions about the regression function to be estimated.  The\nuniversal estimator studied in this paper consists of a least-square\nfitting procedure using piecewise constant functions on a partition\nwhich depends adaptively on the data.  The partition is generated by a\nsplitting procedure which differs from those used in CART algorithms.\nIt is proven that this estimator performs at the optimal convergence\nrate for a wide class of priors on the regression function.  Namely,\nas will be made precise in the text, if the regression function is in\nany one of a certain class of approximation spaces (or smoothness\nspaces of order not exceeding one -- a limitation resulting because\nthe estimator uses piecewise constants) measured relative to the\nmarginal measure, then the estimator converges to the regression\nfunction (in the least squares sense) with an optimal rate of\nconvergence in terms of the number of samples.  The estimator is also\nnumerically feasible and can be implemented on-line.",
    "authors": [
        "Peter Binev",
        "Albert Cohen",
        "Wolfgang Dahmen",
        "Ronald DeVore",
        "Vladimir Temlyakov"
    ],
    "id": "binev05a",
    "issue": 44,
    "pages": [
        1297,
        1321
    ],
    "title": "Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions",
    "volume": "6",
    "year": "2005"
}