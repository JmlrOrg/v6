{
    "abstract": "Bayesian inference is now widely established as one of the\nprincipal foundations for machine learning. In practice, exact\ninference is rarely possible, and so a variety of approximation\ntechniques have been developed, one of the most widely used being\na deterministic framework called variational inference. In this\npaper we introduce Variational Message Passing (VMP), a general\npurpose algorithm for applying variational inference to Bayesian\nNetworks. Like belief propagation, VMP proceeds by sending\nmessages between nodes in the network and updating posterior\nbeliefs using local operations at each node. Each such update\nincreases a lower bound on the log evidence (unless already at a\nlocal maximum). In contrast to belief propagation, VMP can be\napplied to a very general class of conjugate-exponential models\nbecause it uses a factorised variational approximation.\nFurthermore, by introducing additional variational parameters, VMP\ncan be applied to models containing non-conjugate distributions.\nThe VMP framework also allows the lower bound to be evaluated, and\nthis can be used both for model comparison and for detection of\nconvergence. Variational message passing has been implemented in\nthe form of a general purpose inference engine called VIBES\n('Variational Inference for BayEsian networkS') which allows\nmodels to be specified graphically and then solved variationally\nwithout recourse to coding.",
    "authors": [
        "John Winn",
        "Christopher M. Bishop"
    ],
    "id": "winn05a",
    "issue": 23,
    "pages": [
        661,
        694
    ],
    "title": "Variational Message Passing",
    "volume": "6",
    "year": "2005"
}