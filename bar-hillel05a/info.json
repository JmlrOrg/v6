{
    "abstract": "<p>\nMany learning algorithms use a metric defined over the input space as\na principal tool, and their performance critically depends on the\nquality of this metric.  We address the problem of learning metrics\nusing side-information in the form of equivalence constraints.  Unlike\nlabels, we demonstrate that this type of side-information can\nsometimes be automatically obtained without the need of human\nintervention.  We show how such side-information can be used to modify\nthe representation of the data, leading to improved clustering and\nclassification.\n</p>\n<p>\nSpecifically, we present the Relevant Component Analysis (RCA)\nalgorithm, which is a simple and efficient algorithm for learning a\nMahalanobis metric.  We show that RCA is the solution of an\ninteresting optimization problem, founded on an information theoretic\nbasis.  If dimensionality reduction is allowed within RCA, we show\nthat it is optimally accomplished by a version of\nFisher's linear discriminant that uses constraints. Moreover, under certain Gaussian\nassumptions, RCA can be viewed as a Maximum Likelihood estimation of\nthe within class covariance matrix. We conclude with extensive\nempirical evaluations of RCA, showing its advantage over alternative\nmethods.\n</p>",
    "authors": [
        "Aharon Bar-Hillel",
        "Tomer Hertz",
        "Noam Shental",
        "Daphna Weinshall"
    ],
    "id": "bar-hillel05a",
    "issue": 32,
    "pages": [
        937,
        965
    ],
    "title": "Learning a Mahalanobis Metric from Equivalence Constraints",
    "volume": "6",
    "year": "2005"
}