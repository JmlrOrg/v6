{
    "abstract": "Summarising a high dimensional data set with a low dimensional embedding\nis a standard approach for exploring its structure. In this paper\nwe provide an overview of some existing techniques for discovering\nsuch embeddings. We then introduce a novel probabilistic interpretation\nof principal component analysis (PCA) that we term dual probabilistic\nPCA (DPPCA). The DPPCA model has the additional advantage that the\nlinear mappings from the embedded space can easily be non-linearised\nthrough Gaussian processes. We refer to this model as a Gaussian process\nlatent variable model (GP-LVM). Through analysis of the GP-LVM objective\nfunction, we relate the model to popular spectral techniques such\nas kernel PCA and multidimensional scaling. We then review a practical\nalgorithm for GP-LVMs in the context of large data sets and develop\nit to also handle discrete valued data and missing attributes. We\ndemonstrate the model on a range of real-world and artificially generated\ndata sets.",
    "authors": [
        "Neil Lawrence"
    ],
    "id": "lawrence05a",
    "issue": 60,
    "pages": [
        1783,
        1816
    ],
    "title": "Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models",
    "volume": "6",
    "year": "2005"
}