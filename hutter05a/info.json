{
    "abstract": "When applying aggregating strategies to Prediction with Expert\nAdvice (PEA), the learning rate must be adaptively tuned. The\nnatural choice of sqrt(complexity/current loss)\nrenders the analysis of Weighted Majority (WM) derivatives quite\ncomplicated. In particular, for arbitrary weights there have\nbeen no results proven so far. The analysis of the alternative\nFollow the Perturbed Leader (FPL) algorithm from Kalai and\nVempala (2003) based on Hannan's algorithm is easier. We\nderive loss bounds for adaptive learning rate and both finite\nexpert classes with uniform weights and countable expert\nclasses with arbitrary weights. For the former setup, our loss\nbounds match the best known results so far, while for the\nlatter our results are new.",
    "authors": [
        "Marcus Hutter",
        "Jan Poland"
    ],
    "id": "hutter05a",
    "issue": 22,
    "pages": [
        639,
        660
    ],
    "title": "Adaptive Online Prediction by Following the Perturbed Leader",
    "volume": "6",
    "year": "2005"
}